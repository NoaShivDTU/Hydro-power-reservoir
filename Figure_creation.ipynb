{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1ec42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import re\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5eea95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] fig_first_hour_vs_N.pdf — saved to outputs\n",
      "[ok] EEV/WS tables saved to outputs\n",
      "[ok] Out-of-sample distribution figures saved.\n",
      "[warn] Could not build efficiency frontier (missing runtime or OOS).\n",
      "[warn] Could not find policy/level pairs for t=1; skipped policy plot.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def ensure_dir(path: Path):\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def unzip_if_needed(path: Path, work_root: Path) -> Path:\n",
    "    if path.is_dir():\n",
    "        return path\n",
    "    if zipfile.is_zipfile(path):\n",
    "        dest = work_root / path.stem\n",
    "        if dest.exists():\n",
    "            return dest\n",
    "        ensure_dir(dest)\n",
    "        with zipfile.ZipFile(path, \"r\") as zf:\n",
    "            zf.extractall(dest)\n",
    "        return dest\n",
    "    raise FileNotFoundError(f\"Not a directory or zip: {path}\")\n",
    "\n",
    "\n",
    "def debug_inventory(root: Path, label: str):\n",
    "    print(f\"\\n[debug] Scanning {label} at {root}\")\n",
    "    csvs = list(root.rglob(\"*.csv\"))\n",
    "    if not csvs:\n",
    "        print(\"[debug]   No CSV files found.\")\n",
    "        return\n",
    "    for f in sorted(csvs)[:200]:\n",
    "        try:\n",
    "            df = pd.read_csv(f, nrows=2)\n",
    "            cols = \", \".join(map(str, df.columns.tolist()))\n",
    "        except Exception as e:\n",
    "            cols = f\"<could not read: {e}>\"\n",
    "        try:\n",
    "            rel = f.relative_to(root)\n",
    "        except Exception:\n",
    "            rel = f\n",
    "        print(f\"[debug]   {rel} -> cols: {cols}\")\n",
    "\n",
    "\n",
    "# ---------------------------- Loaders ----------------------------\n",
    "\n",
    "def load_runtime_from_adp_runtime_by_N(root: Path) -> Dict[str, np.ndarray]:\n",
    "    cand = list(root.rglob(\"adp_runtime_by_N.csv\"))\n",
    "    if not cand:\n",
    "        return {\"N\": np.array([]), \"runtime_h\": np.array([])}\n",
    "    df = pd.read_csv(cand[0])\n",
    "    if \"N\" in df.columns and \"runtime_hours\" in df.columns:\n",
    "        Ns = pd.to_numeric(df[\"N\"], errors=\"coerce\").dropna().astype(int).to_numpy()\n",
    "        H  = pd.to_numeric(df[\"runtime_hours\"], errors=\"coerce\").dropna().to_numpy()\n",
    "        idx = np.argsort(Ns)\n",
    "        return {\"N\": Ns[idx], \"runtime_h\": H[idx]}\n",
    "    return {\"N\": np.array([]), \"runtime_h\": np.array([])}\n",
    "\n",
    "\n",
    "def load_runtime_from_run_summary(root: Path) -> Dict[str, np.ndarray]:\n",
    "    cand = list(root.rglob(\"run_summary_allN.csv\"))\n",
    "    if not cand:\n",
    "        return {\"N\": np.array([]), \"runtime_h\": np.array([])}\n",
    "    df = pd.read_csv(cand[0])\n",
    "    if \"N\" in df.columns and \"runtime_hours\" in df.columns:\n",
    "        Ns = pd.to_numeric(df[\"N\"], errors=\"coerce\").dropna().astype(int).to_numpy()\n",
    "        H  = pd.to_numeric(df[\"runtime_hours\"], errors=\"coerce\").dropna().to_numpy()\n",
    "        idx = np.argsort(Ns)\n",
    "        return {\"N\": Ns[idx], \"runtime_h\": H[idx]}\n",
    "    return {\"N\": np.array([]), \"runtime_h\": np.array([])}\n",
    "\n",
    "\n",
    "def load_std_from_perN_files(root: Path) -> Dict[int, pd.DataFrame]:\n",
    "    by_N: Dict[int, pd.DataFrame] = {}\n",
    "    for pth in root.rglob(\"adp_N*_std_last5_by_time.csv\"):\n",
    "        m = re.search(r\"adp_N(\\d+)_std_last5_by_time\\.csv$\", pth.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        N = int(m.group(1))\n",
    "        df = pd.read_csv(pth)\n",
    "        row = df.iloc[0]\n",
    "        hour_cols = [c for c in df.columns if isinstance(c, str) and c.lower().startswith(\"hour\")]\n",
    "        if not hour_cols:\n",
    "            hour_cols = df.columns.tolist()\n",
    "        data = []\n",
    "        for c in hour_cols:\n",
    "            m2 = re.search(r'(\\d+)$', str(c))\n",
    "            t = int(m2.group(1)) if m2 else hour_cols.index(c) + 1\n",
    "            v = pd.to_numeric(row[c], errors=\"coerce\")\n",
    "            if pd.isna(v):\n",
    "                continue\n",
    "            data.append((t, float(v)))\n",
    "        if data:\n",
    "            data.sort(key=lambda x: x[0])\n",
    "            by_N[N] = pd.DataFrame(data, columns=[\"time\", \"value\"]).assign(sample_id=0)\n",
    "    return by_N\n",
    "\n",
    "\n",
    "def load_eev_ws_from_pair_files(root: Path) -> Optional[pd.DataFrame]:\n",
    "    rows = []\n",
    "    for p in root.rglob(\"in_sample_N*.csv\"):\n",
    "        m = re.search(r\"in_sample_N(\\d+)\\.csv$\", p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        N = int(m.group(1))\n",
    "        df_in = pd.read_csv(p)\n",
    "        q = p.with_name(f\"out_of_sample_N{N}.csv\")\n",
    "        if not q.exists():\n",
    "            continue\n",
    "        df_out = pd.read_csv(q)\n",
    "        for approach in [\"EEV\", \"WS\", \"ADP\", \"SDDP\"]:\n",
    "            if approach in df_in.columns and approach in df_out.columns:\n",
    "                rows.append({\n",
    "                    \"approach\": approach,\n",
    "                    \"N\": N,\n",
    "                    \"in_sample\": float(pd.to_numeric(df_in[approach], errors=\"coerce\").mean()),\n",
    "                    \"out_of_sample\": float(pd.to_numeric(df_out[approach], errors=\"coerce\").mean()),\n",
    "                })\n",
    "    if not rows:\n",
    "        return None\n",
    "    return pd.DataFrame(rows).sort_values([\"approach\", \"N\"])\n",
    "\n",
    "\n",
    "# --- replace the whole function ---\n",
    "def load_first_hour_auto(root: Path) -> Optional[Tuple[np.ndarray, np.ndarray, str]]:\n",
    "    \"\"\"\n",
    "    Return (x, y, xname). If no level/state column exists, fall back to 'path'\n",
    "    so we at least plot value-by-scenario index (not a true value function).\n",
    "    \"\"\"\n",
    "    candidates = list(root.rglob(\"run_N*_first_hour.csv\"))\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    def N_of(path):\n",
    "        m = re.search(r\"run_N(\\d+)_first_hour\\.csv$\", path.name, re.IGNORECASE)\n",
    "        return int(m.group(1)) if m else -1\n",
    "\n",
    "    candidates.sort(key=lambda q: N_of(q), reverse=True)\n",
    "\n",
    "    for csv_path in candidates:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # y (value)\n",
    "        ycol = None\n",
    "        for cand in [\"value\",\"vbar2_at_xpost1\",\"Vhat_hour1\",\"v_est\",\"vhat\",\"v\"]:\n",
    "            if cand in df.columns:\n",
    "                ycol = cand; break\n",
    "        if ycol is None:\n",
    "            nums = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "            if not nums: \n",
    "                continue\n",
    "            ycol = nums[-1]\n",
    "\n",
    "        # x (level/state) preferred list\n",
    "        xcol = None\n",
    "        preferred = [\"level\",\"post_level\",\"xpost1\",\"x_post\",\"xpost\",\"Reservoir1\",\"Reservoir 1\",\"state\",\"l\",\"storage\"]\n",
    "        for cand in preferred:\n",
    "            if cand in df.columns:\n",
    "                xcol = cand; break\n",
    "\n",
    "        # Fallbacks:\n",
    "        #   1) 'path' if present (scenario index)\n",
    "        #   2) any numeric column ≠ y (even if it's price/inflow/theta)\n",
    "        if xcol is None:\n",
    "            if \"path\" in df.columns:\n",
    "                xcol = \"path\"\n",
    "            else:\n",
    "                nums = [c for c in df.select_dtypes(include=[\"number\"]).columns if c != ycol]\n",
    "                if not nums:\n",
    "                    continue\n",
    "                xcol = max(nums, key=lambda c: pd.to_numeric(df[c], errors=\"coerce\").var())\n",
    "\n",
    "        x = pd.to_numeric(df[xcol], errors=\"coerce\").dropna().to_numpy()\n",
    "        y = pd.to_numeric(df[ycol], errors=\"coerce\").dropna().to_numpy()\n",
    "        if len(x) and len(y):\n",
    "            return (x, y, xcol)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- New helpers for 4/5/6/7 ----------------------------\n",
    "\n",
    "def write_eev_ws_table_tex(eevws_df: pd.DataFrame, out_tex: Path):\n",
    "    df = eevws_df.copy()\n",
    "    Ns = sorted(df[\"N\"].unique())\n",
    "    lines = []\n",
    "    lines.append(r\"\\begin{table}[ht]\")\n",
    "    lines.append(r\"\\centering\")\n",
    "    lines.append(r\"\\caption{Mean profit across in-sample and out-of-sample scenarios.}\")\n",
    "    lines.append(r\"\\begin{tabular}{l\" + \"c\" * (2 * len(Ns)) + \"}\")\n",
    "    lines.append(r\"\\toprule\")\n",
    "    header = [\"Approach\"] + [f\"In-sample ($N={n}$)\" for n in Ns] + [f\"Out-of-sample ($N={n}$)\" for n in Ns]\n",
    "    lines.append(\" & \".join(header) + r\" \\\\\")\n",
    "    lines.append(r\"\\midrule\")\n",
    "    for approach in [\"EEV\", \"WS\", \"ADP\", \"SDDP\"]:\n",
    "        if approach not in df[\"approach\"].unique():\n",
    "            continue\n",
    "        row_in, row_out = [], []\n",
    "        for n in Ns:\n",
    "            sub = df[(df[\"approach\"] == approach) & (df[\"N\"] == n)]\n",
    "            if sub.empty:\n",
    "                row_in.append(\"\")\n",
    "                row_out.append(\"\")\n",
    "            else:\n",
    "                row_in.append(f\"{pd.to_numeric(sub['in_sample']).values[0]:.4f}\")\n",
    "                row_out.append(f\"{pd.to_numeric(sub['out_of_sample']).values[0]:.4f}\")\n",
    "        lines.append(\" & \".join([approach] + row_in + row_out) + r\" \\\\\")\n",
    "    lines.append(r\"\\bottomrule\")\n",
    "    lines.append(r\"\\end{tabular}\")\n",
    "    lines.append(r\"\\label{tab:eev_ws}\")\n",
    "    lines.append(r\"\\end{table}\")\n",
    "    out_tex.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def load_oos_stack(eev_root: Path) -> Optional[pd.DataFrame]:\n",
    "    rows = []\n",
    "    for pth in eev_root.rglob(\"out_of_sample_N*.csv\"):\n",
    "        m = re.search(r\"out_of_sample_N(\\d+)\\.csv$\", pth.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        N = int(m.group(1))\n",
    "        df = pd.read_csv(pth)\n",
    "        for col in df.columns:\n",
    "            if col.strip().lower() in {\"scenario\"}:\n",
    "                continue\n",
    "            series = pd.to_numeric(df[col], errors=\"coerce\").dropna()\n",
    "            for v in series.values:\n",
    "                rows.append({\"N\": N, \"approach\": col, \"value\": float(v)})\n",
    "    if not rows:\n",
    "        return None\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_oos_distributions_by_N(oos_long: pd.DataFrame, outdir: Path):\n",
    "    for N, grp in oos_long.groupby(\"N\"):\n",
    "        plt.figure(figsize=(6.0, 4.0), dpi=150)\n",
    "        approaches = sorted(grp[\"approach\"].unique())\n",
    "        data = [grp[grp[\"approach\"] == a][\"value\"].to_numpy() for a in approaches]\n",
    "        plt.boxplot(data, labels=approaches, showfliers=False)\n",
    "        plt.xlabel(\"Approach\")\n",
    "        plt.ylabel(\"Out-of-sample profit ($)\")\n",
    "        plt.title(f\"Out-of-sample distributions (N={N})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outdir / f\"fig_oos_dist_N{N}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def load_runtime_generic(root: Path) -> Dict[str, np.ndarray]:\n",
    "    rt = load_runtime_from_adp_runtime_by_N(root)\n",
    "    if not rt[\"N\"].size:\n",
    "        rt = load_runtime_from_run_summary(root)\n",
    "    return rt\n",
    "\n",
    "\n",
    "def compute_efficiency_points(runtime_root: Path, oos_long: Optional[pd.DataFrame], approaches: List[str]) -> Optional[pd.DataFrame]:\n",
    "    if oos_long is None:\n",
    "        return None\n",
    "    rows = []\n",
    "    rt = load_runtime_generic(runtime_root)\n",
    "    if not rt[\"N\"].size:\n",
    "        return None\n",
    "    rt_map = {int(n): float(h) for n, h in zip(rt[\"N\"], rt[\"runtime_h\"])}\n",
    "    for approach in approaches:\n",
    "        sub = oos_long[oos_long[\"approach\"].str.lower() == approach.lower()]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        for N, grp in sub.groupby(\"N\"):\n",
    "            quality = float(pd.to_numeric(grp[\"value\"], errors=\"coerce\").mean())\n",
    "            rh = rt_map.get(int(N))\n",
    "            if rh is None:\n",
    "                continue\n",
    "            rows.append({\"approach\": approach.upper(), \"N\": int(N), \"runtime_h\": rh, \"quality\": quality})\n",
    "    if not rows:\n",
    "        return None\n",
    "    return pd.DataFrame(rows).sort_values([\"approach\",\"N\"])\n",
    "\n",
    "\n",
    "def plot_efficiency_frontier(points_df: pd.DataFrame, out: Path):\n",
    "    plt.figure(figsize=(6.0, 4.0), dpi=150)\n",
    "    for appr, grp in points_df.groupby(\"approach\"):\n",
    "        grp = grp.sort_values(\"runtime_h\")\n",
    "        plt.plot(grp[\"runtime_h\"], grp[\"quality\"], marker=\"o\", label=appr)\n",
    "        for _, r in grp.iterrows():\n",
    "            plt.annotate(str(int(r[\"N\"])), (r[\"runtime_h\"], r[\"quality\"]), xytext=(3,3), textcoords=\"offset points\")\n",
    "    plt.xlabel(\"Runtime (h)\")\n",
    "    plt.ylabel(\"Out-of-sample mean profit ($)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def load_policy_t1_pair(root: Path) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n",
    "    # levels\n",
    "    lev = None\n",
    "    for pth in root.rglob(\"*lastpath*_levels.csv\"):\n",
    "        lev = pth; break\n",
    "    if lev is None:\n",
    "        return None\n",
    "    try:\n",
    "        dL = pd.read_csv(lev)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # time column (or fall back to first row)\n",
    "    tcol = next((c for c in dL.columns if c.strip().lower() in {\"time\",\"hour\",\"t\",\"stage\"}), None)\n",
    "    if tcol is not None:\n",
    "        Lrow = dL[dL[tcol] == dL[tcol].min()]\n",
    "    else:\n",
    "        Lrow = dL.head(1)\n",
    "\n",
    "    level_cols = [c for c in Lrow.select_dtypes(include=[\"number\"]).columns if c != tcol]\n",
    "    if not level_cols:\n",
    "        return None\n",
    "    level_t1 = Lrow[level_cols[0]].to_numpy()\n",
    "\n",
    "    # decisions\n",
    "    pi = None\n",
    "    for pth in root.rglob(\"*lastpath*_pi.csv\"):\n",
    "        pi = pth; break\n",
    "    if pi is None:\n",
    "        return None\n",
    "    try:\n",
    "        dP = pd.read_csv(pi)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    tcolP = next((c for c in dP.columns if c.strip().lower() in {\"time\",\"hour\",\"t\",\"stage\"}), None)\n",
    "    if tcolP is not None:\n",
    "        Prow = dP[dP[tcolP] == dP[tcolP].min()]\n",
    "    else:\n",
    "        Prow = dP.head(1)\n",
    "\n",
    "    dec_cols = [c for c in Prow.select_dtypes(include=[\"number\"]).columns if c != tcolP]\n",
    "    if not dec_cols:\n",
    "        return None\n",
    "    # pick the most varying numeric as decision if multiple\n",
    "    dec_col = max(dec_cols, key=lambda c: pd.to_numeric(dP[c], errors=\"coerce\").var())\n",
    "    pi_t1 = Prow[dec_col].to_numpy()\n",
    "\n",
    "    if level_t1.size and pi_t1.size:\n",
    "        return (level_t1, pi_t1)\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def plot_policy_t1(adp_pair: Optional[Tuple[np.ndarray, np.ndarray]], sddp_pair: Optional[Tuple[np.ndarray, np.ndarray]], out: Path):\n",
    "    if adp_pair is None and sddp_pair is None:\n",
    "        return\n",
    "    plt.figure(figsize=(6.0, 4.0), dpi=150)\n",
    "    if adp_pair is not None and adp_pair[0].size and adp_pair[1].size:\n",
    "        plt.plot(adp_pair[0], adp_pair[1], marker=\"o\", linestyle=\"None\", label=\"ADP\")\n",
    "    if sddp_pair is not None and sddp_pair[0].size and sddp_pair[1].size:\n",
    "        plt.plot(sddp_pair[0], sddp_pair[1], marker=\"+\", linestyle=\"None\", label=\"SDDP\")\n",
    "    plt.xlabel(\"Level at t=1\")\n",
    "    plt.ylabel(\"Dispatch / decision at t=1\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ---------------------------- Plotters ----------------------------\n",
    "\n",
    "def plot_runtime(adp_rt: Dict[str, np.ndarray], sddp_rt: Dict[str, np.ndarray], out: Path):\n",
    "    plt.figure(figsize=(6.0, 4.0), dpi=150)\n",
    "    if adp_rt[\"N\"].size:\n",
    "        plt.plot(adp_rt[\"N\"], adp_rt[\"runtime_h\"], marker=\"*\", linestyle=\"None\", label=\"ADP\")\n",
    "    if sddp_rt[\"N\"].size:\n",
    "        plt.plot(sddp_rt[\"N\"], sddp_rt[\"runtime_h\"], marker=\"+\", linestyle=\"None\", label=\"SDDP\")\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.ylabel(\"Running time (h)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_std_last5(by_N: Dict[int, pd.DataFrame], out: Path):\n",
    "    plt.figure(figsize=(6.0, 4.5), dpi=150)\n",
    "    for n in sorted(by_N.keys()):\n",
    "        df = by_N[n].sort_values(\"time\")\n",
    "        plt.plot(df[\"time\"], df[\"value\"], label=f\"{n} samples\")\n",
    "    plt.xlabel(\"Time (h)\")\n",
    "    plt.ylabel(\"Standard deviation of last five samples ($)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# --- replace the whole function ---\n",
    "def plot_first_hour_curve(adp_curve, sddp_curve, out: Path):\n",
    "    \"\"\"\n",
    "    adp_curve / sddp_curve are (x, y, xname) or None.\n",
    "    Chooses the x-label based on xname: if it's 'path' we say 'Scenario index'.\n",
    "    \"\"\"\n",
    "    if adp_curve is None and sddp_curve is None:\n",
    "        return\n",
    "\n",
    "    def label_for_x(xname: str) -> str:\n",
    "        if xname is None:\n",
    "            return \"X\"\n",
    "        xname_l = xname.strip().lower()\n",
    "        if xname_l == \"path\":\n",
    "            return \"Scenario index at t=1\"\n",
    "        # looks like a level/state name\n",
    "        return \"Post-decision level at t=1\"\n",
    "\n",
    "    # pick x label from whichever we have\n",
    "    xl = \"X\"\n",
    "    for c in (adp_curve, sddp_curve):\n",
    "        if c is not None:\n",
    "            xl = label_for_x(c[2]); break\n",
    "\n",
    "    plt.figure(figsize=(6.0, 4.0), dpi=150)\n",
    "    if adp_curve is not None:\n",
    "        x, y, _ = adp_curve\n",
    "        idx = np.argsort(x)\n",
    "        plt.plot(np.array(x)[idx], np.array(y)[idx], label=\"ADP\")\n",
    "    if sddp_curve is not None:\n",
    "        x, y, _ = sddp_curve\n",
    "        idx = np.argsort(x)\n",
    "        plt.plot(np.array(x)[idx], np.array(y)[idx], label=\"SDDP\")\n",
    "\n",
    "    plt.xlabel(xl)\n",
    "    plt.ylabel(\"Value function at t=1 ($)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def find_first_hour_value_col(df: pd.DataFrame) -> str:\n",
    "    for c in [\"vbar2_at_xpost1\",\"value\",\"Vhat_hour1\",\"v_est\",\"vhat\",\"v\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    # fallback: last numeric col\n",
    "    nums = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    return nums[-1] if nums else None\n",
    "\n",
    "def load_first_hour_agg_by_N(root: Path, agg: str = \"mean\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    For each run_N*_first_hour.csv under `root`, compute an aggregate (mean/median)\n",
    "    of the first-hour value column and return aligned arrays (N, value).\n",
    "    \"\"\"\n",
    "    files = list(root.rglob(\"run_N*_first_hour.csv\"))\n",
    "    rows = []\n",
    "    for f in files:\n",
    "        m = re.search(r\"run_N(\\d+)_first_hour\\.csv$\", f.name, re.IGNORECASE)\n",
    "        if not m: \n",
    "            continue\n",
    "        N = int(m.group(1))\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "        except Exception:\n",
    "            continue\n",
    "        ycol = find_first_hour_value_col(df)\n",
    "        if not ycol: \n",
    "            continue\n",
    "        vals = pd.to_numeric(df[ycol], errors=\"coerce\").dropna()\n",
    "        if vals.empty:\n",
    "            continue\n",
    "        y = (vals.mean() if agg.lower()==\"mean\" else vals.median())\n",
    "        rows.append((N, float(y)))\n",
    "    if not rows:\n",
    "        return np.array([]), np.array([])\n",
    "    rows.sort(key=lambda t: t[0])\n",
    "    Ns, Ys = zip(*rows)\n",
    "    return np.array(Ns, dtype=int), np.array(Ys, dtype=float)\n",
    "\n",
    "def plot_first_hour_vs_N(adp_xy: Tuple[np.ndarray, np.ndarray],\n",
    "                         sddp_xy: Tuple[np.ndarray, np.ndarray],\n",
    "                         out: Path,\n",
    "                         agg_label: str = \"mean\"):\n",
    "    \"\"\"\n",
    "    Plot first-hour value vs number of samples N with twin y-axes:\n",
    "    - left y-axis: ADP\n",
    "    - right y-axis: SDDP\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(6.0, 4.0), dpi=150)\n",
    "\n",
    "    # Left axis for ADP\n",
    "    Ns, Vy = adp_xy\n",
    "    if Ns.size:\n",
    "        ax1.plot(Ns, Vy, marker=\"*\", color=\"tab:blue\", label=\"ADP\")\n",
    "        ax1.set_ylabel(f\"ADP first-hour value ({agg_label}) ($)\", color=\"tab:blue\")\n",
    "        ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "\n",
    "    # Right axis for SDDP\n",
    "    ax2 = ax1.twinx()\n",
    "    Ns2, Vy2 = sddp_xy\n",
    "    if Ns2.size:\n",
    "        ax2.plot(Ns2, Vy2, marker=\"+\", color=\"tab:orange\", label=\"SDDP\")\n",
    "        ax2.set_ylabel(f\"SDDP first-hour value ({agg_label}) ($)\", color=\"tab:orange\")\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=\"tab:orange\")\n",
    "\n",
    "    ax1.set_xlabel(\"Number of samples\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ---------------------------- Main ----------------------------\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--adp\", default=\"adp_results\")\n",
    "    parser.add_argument(\"--sddp\", default=\"SDDP_results\")\n",
    "    parser.add_argument(\"--eevws\", default=\"results_eev_ws\")\n",
    "    parser.add_argument(\"--outdir\", default=\"outputs\")\n",
    "    parser.add_argument(\"--debug\", action=\"store_true\")\n",
    "    parser.add_argument(\n",
    "    \"--firsthour_agg\",\n",
    "    nargs=\"?\",           # value is optional\n",
    "    const=\"mean\",        # if provided without a value, use 'mean'\n",
    "    default=\"mean\",\n",
    "    choices=[\"mean\",\"median\"],\n",
    "    help=\"Aggregation over scenarios for first-hour plot vs N\"\n",
    ")\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    work_root = Path(\".work\"); ensure_dir(work_root)\n",
    "    outdir = Path(args.outdir); ensure_dir(outdir)\n",
    "\n",
    "    def resolve(pstr): return unzip_if_needed(Path(pstr), work_root)\n",
    "\n",
    "    adp_root, sddp_root, eev_root = resolve(args.adp), resolve(args.sddp), resolve(args.eevws)\n",
    "\n",
    "    if args.debug:\n",
    "        debug_inventory(adp_root,\"ADP root\")\n",
    "        debug_inventory(sddp_root,\"SDDP root\")\n",
    "        debug_inventory(eev_root,\"EEV/WS root\")\n",
    "\n",
    "    # Runtime\n",
    "    def load_runtime_generic(root: Path) -> Dict[str, np.ndarray]:\n",
    "        rt = load_runtime_from_adp_runtime_by_N(root)\n",
    "        if not rt[\"N\"].size: rt = load_runtime_from_run_summary(root)\n",
    "        return rt\n",
    "\n",
    "    adp_rt = load_runtime_generic(adp_root)\n",
    "    sddp_rt = load_runtime_generic(sddp_root)\n",
    "    plot_runtime(adp_rt, sddp_rt, outdir/\"fig_runtime.pdf\")\n",
    "    \n",
    "    # --- First-hour value vs N (ADP vs SDDP) ---\n",
    "    adp_N, adp_V = load_first_hour_agg_by_N(adp_root, agg=args.firsthour_agg)\n",
    "    sddp_N, sddp_V = load_first_hour_agg_by_N(sddp_root, agg=args.firsthour_agg)\n",
    "    plot_first_hour_vs_N((adp_N, adp_V), (sddp_N, sddp_V),\n",
    "                        outdir / \"fig_first_hour_vs_N.pdf\",\n",
    "                        agg_label=args.firsthour_agg)\n",
    "    print(f\"[ok] fig_first_hour_vs_N.pdf — saved to {outdir}\")\n",
    "\n",
    "\n",
    "    # Std curves\n",
    "    by_N = load_std_from_perN_files(adp_root)\n",
    "    if by_N:\n",
    "        plot_std_last5(by_N, outdir/\"fig_std_last5.pdf\")\n",
    "\n",
    "    # First hour\n",
    "    adp_curve = load_first_hour_auto(adp_root)\n",
    "    sddp_curve = load_first_hour_auto(sddp_root)\n",
    "    plot_first_hour_curve(adp_curve, sddp_curve, outdir/\"fig_first_hour_value.pdf\")\n",
    "\n",
    "    # EEV/WS tables\n",
    "    eevws = load_eev_ws_from_pair_files(eev_root)\n",
    "    if eevws is not None:\n",
    "        eevws.to_csv(outdir/\"eev_ws_table.csv\", index=False)\n",
    "        write_eev_ws_table_tex(eevws, outdir/\"table_eev_ws.tex\")\n",
    "        print(f\"[ok] EEV/WS tables saved to {outdir}\")\n",
    "    else:\n",
    "        print(\"[warn] Could not load EEV/WS results; skipped table.\")\n",
    "\n",
    "    # OOS distributions\n",
    "    oos_long = load_oos_stack(eev_root)\n",
    "    if oos_long is not None:\n",
    "        plot_oos_distributions_by_N(oos_long, outdir)\n",
    "        print(\"[ok] Out-of-sample distribution figures saved.\")\n",
    "    else:\n",
    "        print(\"[warn] No out-of-sample files found; skipped distributions.\")\n",
    "\n",
    "    # Efficiency frontier\n",
    "  # Efficiency frontier (quality vs runtime)\n",
    "    oos_long = load_oos_stack(eev_root)  # keep this\n",
    "    # Build points per approach with the right runtime roots\n",
    "    points = []\n",
    "    for approach, rroot in [(\"ADP\", adp_root), (\"SDDP\", sddp_root), (\"EEV\", eev_root), (\"WS\", eev_root)]:\n",
    "        sub = oos_long[oos_long[\"approach\"].str.lower() == approach.lower()] if oos_long is not None else None\n",
    "        if sub is None or sub.empty:\n",
    "            continue\n",
    "        rt = load_runtime_from_adp_runtime_by_N(rroot)\n",
    "        if not rt[\"N\"].size:\n",
    "            rt = load_runtime_from_run_summary(rroot)\n",
    "        if not rt[\"N\"].size:\n",
    "            continue\n",
    "        rt_map = {int(n): float(h) for n, h in zip(rt[\"N\"], rt[\"runtime_h\"])}\n",
    "        for N, grp in sub.groupby(\"N\"):\n",
    "            q = float(pd.to_numeric(grp[\"value\"], errors=\"coerce\").mean())\n",
    "            rh = rt_map.get(int(N))\n",
    "            if rh is None:\n",
    "                continue\n",
    "            points.append({\"approach\": approach, \"N\": int(N), \"runtime_h\": rh, \"quality\": q})\n",
    "\n",
    "    if points:\n",
    "        eff_points = pd.DataFrame(points)\n",
    "        plot_efficiency_frontier(eff_points, outdir/\"fig_efficiency_frontier.pdf\")\n",
    "        print(f\"[ok] fig_efficiency_frontier.pdf — saved to {outdir}\")\n",
    "    else:\n",
    "        print(\"[warn] Could not build efficiency frontier (missing runtime or OOS).\")\n",
    "\n",
    "    # Policy t=1 (dispatch vs level)\n",
    "    adp_pair = load_policy_t1_pair(adp_root)\n",
    "    sddp_pair = load_policy_t1_pair(sddp_root)\n",
    "    if adp_pair or sddp_pair:\n",
    "        plot_policy_t1(adp_pair, sddp_pair, outdir/\"fig_policy_t1_pi_vs_level.pdf\")\n",
    "        print(f\"[ok] fig_policy_t1_pi_vs_level.pdf — saved to {outdir}\")\n",
    "    else:\n",
    "        print(\"[warn] Could not find policy/level pairs for t=1; skipped policy plot.\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    sys.argv = [sys.argv[0]] + [\n",
    "        a for a in sys.argv[1:]\n",
    "        if not (a.startswith(\"-f=\") or a.startswith(\"--f=\"))\n",
    "    ]\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fbf534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
